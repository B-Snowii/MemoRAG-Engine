"""
RAGAS Evaluation Script for MemoRAG-Engine
Professional RAG system evaluation based on official documentation
Reference: https://docs.ragas.io/en/stable/getstarted/rag_eval/
"""

import pandas as pd
import numpy as np
from typing import List, Dict
import json
from datetime import datetime

try:
    import ragas
    from ragas import evaluate, EvaluationDataset
    from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness
    from ragas.llms import LangchainLLMWrapper
    RAGAS_AVAILABLE = True
except ImportError:
    RAGAS_AVAILABLE = False

class MemoRAGEvaluator:
    """MemoRAGç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self, rag_system):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            rag_system: MemoRAGç³»ç»Ÿå®ä¾‹
        """
        self.rag_system = rag_system
        self.evaluation_results = []
        
    def create_test_dataset(self) -> List[Dict]:
        """åˆ›å»ºESGæµ‹è¯•æ•°æ®é›†"""
        test_questions = [
            {
                "question": "What were Alcoa Corp's nitrogen oxide emissions in 2007?",
                "ground_truth": "Alcoa Corp reported nitrogen oxide emissions of 32.8 kilotons in 2007.",
                "contexts": ["Alcoa Corp 2007 NOx emissions data"]
            },
            {
                "question": "Tell me about Alcoa Corp's carbon dioxide emissions performance in 2010",
                "ground_truth": "Alcoa Corp's carbon dioxide emissions in 2010 were 29.5 units, showing a 13.5% increase from 2009.",
                "contexts": ["Alcoa Corp 2010 CO2 emissions data"]
            },
            {
                "question": "How did Agilent Technologies Inc perform in women workforce percentage in 2015?",
                "ground_truth": "Agilent Technologies Inc had specific women workforce percentage data for 2015.",
                "contexts": ["Agilent Technologies Inc 2015 workforce data"]
            },
            {
                "question": "What is the trend of environmental emissions for industrial companies?",
                "ground_truth": "Environmental emissions trends vary by company and year, with some companies showing reduction efforts.",
                "contexts": ["Environmental emissions trend data"]
            },
            {
                "question": "Compare Alcoa Corp emissions between 2007 and 2010",
                "ground_truth": "Alcoa Corp's emissions changed between 2007 and 2010, with specific values for each year.",
                "contexts": ["Alcoa Corp emissions comparison data"]
            }
        ]
        
        return test_questions
    
    def evaluate_single_query(self, question: str, ground_truth: str) -> Dict:
        """è¯„ä¼°å•ä¸ªæŸ¥è¯¢"""
        try:
            result = self.rag_system.process_query(question)
            
            answer = result.get('answer', '')
            contexts = result.get('contexts', [])
            retrieved_docs = result.get('retrieved_docs', [])
            
            evaluation_result = {
                'question': question,
                'ground_truth': ground_truth,
                'answer': answer,
                'contexts': contexts,
                'retrieved_docs': retrieved_docs,
                'timestamp': datetime.now().isoformat()
            }
            
            return evaluation_result
            
        except Exception as e:
            return {
                'question': question,
                'ground_truth': ground_truth,
                'answer': '',
                'contexts': [],
                'retrieved_docs': [],
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def run_ragas_evaluation(self, test_data: List[Dict]) -> Dict:
        """è¿è¡ŒRAGASè¯„ä¼°"""
        if not RAGAS_AVAILABLE:
            return {"error": "RAGAS not available. Install with: pip install ragas"}
        
        try:
            ragas_data = []
            for item in test_data:
                result = self.evaluate_single_query(item['question'], item['ground_truth'])
                
                ragas_item = {
                    'user_input': item['question'],
                    'retrieved_contexts': result['contexts'],
                    'response': result['answer'],
                    'reference': item['ground_truth']
                }
                ragas_data.append(ragas_item)
            
            evaluation_dataset = EvaluationDataset.from_list(ragas_data)
            
            result = evaluate(
                dataset=evaluation_dataset,
                metrics=[
                    LLMContextRecall(),
                    Faithfulness(), 
                    FactualCorrectness()
                ]
            )
            
            return result
            
        except Exception as e:
            return {"error": f"RAGAS evaluation failed: {str(e)}"}
    
    def manual_evaluation(self, test_data: List[Dict]) -> Dict:
        """æ‰‹åŠ¨è¯„ä¼°ï¼ˆå½“RAGASä¸å¯ç”¨æ—¶ï¼‰"""
        results = []
        
        for item in test_data:
            result = self.evaluate_single_query(item['question'], item['ground_truth'])
            results.append(result)
        
        total_questions = len(results)
        successful_answers = len([r for r in results if r.get('answer', '') != ''])
        error_count = len([r for r in results if 'error' in r])
        
        evaluation_summary = {
            'total_questions': total_questions,
            'successful_answers': successful_answers,
            'success_rate': successful_answers / total_questions if total_questions > 0 else 0,
            'error_count': error_count,
            'error_rate': error_count / total_questions if total_questions > 0 else 0,
            'detailed_results': results
        }
        
        return evaluation_summary
    
    def save_evaluation_results(self, results: Dict, filename: str = None):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ragas_evaluation_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"Evaluation results saved to: {filename}")
    
    def run_full_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("ğŸš€ Starting MemoRAG-Engine Evaluation...")
        
        test_data = self.create_test_dataset()
        print(f"ğŸ“Š Created {len(test_data)} test questions")
        
        if RAGAS_AVAILABLE:
            print("ğŸ” Running RAGAS evaluation...")
            ragas_results = self.run_ragas_evaluation(test_data)
            
            if 'error' not in ragas_results:
                print("âœ… RAGAS evaluation completed successfully!")
                print(f"ğŸ“ˆ RAGAS Results: {ragas_results}")
                self.save_evaluation_results(ragas_results, "ragas_evaluation_results.json")
            else:
                print(f"âŒ RAGAS evaluation failed: {ragas_results['error']}")
        
        print("ğŸ” Running manual evaluation...")
        manual_results = self.manual_evaluation(test_data)
        print("âœ… Manual evaluation completed!")
        print(f"ğŸ“ˆ Manual Results: {manual_results}")
        
        self.save_evaluation_results(manual_results, "manual_evaluation_results.json")
        
        return manual_results

def main():
    """ä¸»å‡½æ•°"""
    print("MemoRAG-Engine RAGAS Evaluation")
    print("=" * 50)
    
    print("\nğŸ“‹ To use this evaluation script:")
    print("1. Install RAGAS: pip install ragas")
    print("2. Import your RAG system")
    print("3. Create evaluator instance")
    print("4. Run evaluation")
    
    print("\nğŸ¯ RAGAS Metrics Explained:")
    print("- Context Recall: How well retrieved context covers ground truth")
    print("- Faithfulness: How faithful is the response to retrieved context")
    print("- Factual Correctness: How factually correct is the response")

if __name__ == "__main__":
    main()