#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Fixed Interactive MemoRAG System
Resolves Chroma query syntax issues
"""

import chromadb
from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
import json
import re
from datetime import datetime
from collections import defaultdict
import pickle
import os
import requests
import time

class LLMResponseGenerator:
    """LLM Response Generator"""
    
    def __init__(self, api_key: str = None):
        """Initialize LLM Response Generator"""
        self.api_key = api_key
        self.base_url = "https://api.deepseek.com/v1/chat/completions"
        self.language = "en"  # Default to English
        
    def generate_response(self, query: str, results: List[Dict], context: str = "") -> str:
        """Generate intelligent response"""
        try:
            if not self.api_key:
                return self._fallback_response(query, results)
            
            # Build prompt
            prompt = self._build_prompt(query, results, context)
            
            # Call API (with retry mechanism)
            response = self._call_api_with_retry(prompt)
            
            return response
            
        except Exception as e:
            print(f"LLM call failed: {str(e)}")
            print("ğŸ”„ Switching to basic response mode...")
            return self._fallback_response(query, results)
    
    def _call_api_with_retry(self, prompt: str, max_retries: int = 3) -> str:
        """API call with retry mechanism"""
        for attempt in range(max_retries):
            try:
                return self._call_api(prompt)
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"ğŸ”„ API call failed, retrying {attempt + 1}/{max_retries}: {str(e)}")
                    time.sleep(2)  # Wait 2 seconds before retry
                else:
                    raise e
    
    def _build_prompt(self, query: str, results: List[Dict], context: str) -> str:
        """Build prompt"""
        # Extract key data
        companies = list(set([r['esg_info'].get('company', '') for r in results if r['esg_info'].get('company')]))
        years = list(set([r['esg_info'].get('year', '') for r in results if r['esg_info'].get('year')]))
        indicators = list(set([r['esg_info'].get('indicator', '') for r in results if r['esg_info'].get('indicator')]))
        
        # Build data summary (prioritize valid data)
        data_summary = []
        valid_data_count = 0
        
        for i, r in enumerate(results[:10], 1):  # Show more data for LLM analysis
            esg_info = r['esg_info']
            value = esg_info.get('value', 'N/A')
            rerank_score = r.get('rerank_score', 0)
            
            # Mark data quality
            if value and str(value).lower() != 'nan' and str(value).strip():
                data_quality = "âœ…Valid data"
                valid_data_count += 1
            else:
                data_quality = "âŒMissing data"
            
            data_summary.append(f"{i}. {data_quality} {esg_info.get('company', 'N/A')} ({esg_info.get('year', 'N/A')}) - {esg_info.get('indicator', 'N/A')}: {value} [Quality score:{rerank_score:.1f}]")
        
        # Add data quality statistics
        data_summary.append(f"\nData quality statistics: {valid_data_count} valid data, {len(results) - valid_data_count} missing data")
        
        # Determine response language based on language setting
        response_language = "English"
        if self.language == 'zh':
            response_language = "Chinese (ä¸­æ–‡)"
        
        prompt = f"""You are a professional ESG data analyst. Please generate a professional, accurate, and understandable response based on the following query and data.

Query: {query}

Data Summary (sorted by quality, âœ… indicates valid data, âŒ indicates missing data):
{chr(10).join(data_summary)}

Context: {context}

Important Notes:
- Prioritize using âœ… valid data for analysis
- For âŒ missing data, clearly explain the data gaps
- If valid data is insufficient, explain analysis limitations

Please generate a response that includes the following elements:
1. Direct answer to user's question (based on valid data)
2. Analysis of data trends and patterns (focus on valid data)
3. Provide professional insights
4. Clearly explain data gaps and reasons
5. Give recommendations or conclusions

Response Requirements:
- Use {response_language}
- Professional but understandable
- Based on valid data facts
- Clearly distinguish between valid and missing data
- Clear structure
- Appropriate length (200-400 words)

Response:"""
        
        return prompt
    
    def _call_api(self, prompt: str) -> str:
        """Call API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 1000,
            "temperature": 0.7
        }
        
        try:
            response = requests.post(self.base_url, headers=headers, json=data, timeout=60)
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                raise Exception(f"API call failed: {response.status_code}, {response.text}")
        except requests.exceptions.Timeout:
            raise Exception("API call timeout, please check network connection")
        except requests.exceptions.ConnectionError:
            raise Exception("Network connection error, please check network")
        except Exception as e:
            raise Exception(f"API call exception: {str(e)}")
    
    def _fallback_response(self, query: str, results: List[Dict]) -> str:
        """Fallback response generation"""
        if not results:
            return "Sorry, no data related to your query was found. Please try adjusting your query conditions or check if the data exists."
        
        # Extract key information
        companies = list(set([r['esg_info'].get('company', '') for r in results if r['esg_info'].get('company')]))
        years = list(set([r['esg_info'].get('year', '') for r in results if r['esg_info'].get('year')]))
        indicators = list(set([r['esg_info'].get('indicator', '') for r in results if r['esg_info'].get('indicator')]))
        
        # Analyze data quality
        complete_data = [r for r in results if r['esg_info'].get('value') and str(r['esg_info'].get('value')).lower() != 'nan']
        missing_data = len(results) - len(complete_data)
        
        # Build intelligent response
        response_parts = []
        
        # Opening
        if 'è¶‹åŠ¿' in query or 'trend' in query.lower():
            response_parts.append("æ ¹æ®è¶‹åŠ¿åˆ†æï¼Œ")
        elif 'æ¯”è¾ƒ' in query or 'å¯¹æ¯”' in query or 'vs' in query.lower():
            response_parts.append("é€šè¿‡å¯¹æ¯”åˆ†æï¼Œ")
        elif 'å…·ä½“' in query or 'è¯¦ç»†' in query:
            response_parts.append("å…·ä½“æ•°æ®æ˜¾ç¤ºï¼Œ")
        else:
            response_parts.append("æ ¹æ®æŸ¥è¯¢ç»“æœï¼Œ")
        
        # æ•°æ®ç»Ÿè®¡
        response_parts.append(f"æˆ‘æ‰¾åˆ°äº† {len(results)} æ¡ç›¸å…³æ•°æ®ã€‚")
        
        if companies:
            response_parts.append(f"æ¶‰åŠçš„å…¬å¸åŒ…æ‹¬: {', '.join(companies[:3])}ã€‚")
        
        if years:
            response_parts.append(f"æ•°æ®å¹´ä»½èŒƒå›´: {min(years)}-{max(years)}ã€‚")
        
        if indicators:
            response_parts.append(f"ä¸»è¦æŒ‡æ ‡åŒ…æ‹¬: {', '.join(indicators[:3])}ã€‚")
        
        # æ•°æ®è´¨é‡åˆ†æ
        if missing_data > 0:
            response_parts.append(f"æ³¨æ„ï¼šæœ‰ {missing_data} æ¡æ•°æ®ç¼ºå¤±æˆ–ä¸ºNaNå€¼ã€‚")
        
        # è¶‹åŠ¿åˆ†æ
        if len(results) > 1 and years and len(years) > 1:
            response_parts.append("ä»æ—¶é—´ç»´åº¦çœ‹ï¼Œæ•°æ®å‘ˆç°ä¸€å®šçš„å˜åŒ–è¶‹åŠ¿ã€‚")
        
        # ä¸“ä¸šå»ºè®®
        if missing_data > len(results) * 0.5:
            response_parts.append("å»ºè®®ï¼šæ•°æ®ç¼ºå¤±è¾ƒå¤šï¼Œå»ºè®®æ‰©å¤§æ•°æ®æºæˆ–è°ƒæ•´æŸ¥è¯¢æ¡ä»¶ã€‚")
        else:
            response_parts.append("å»ºè®®ï¼šå¯ä»¥åŸºäºç°æœ‰æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥åˆ†æã€‚")
        
        return " ".join(response_parts)

class OptimizedQueryProcessor:
    """Optimized Query Processor"""
    
    def __init__(self):
        """Initialize Query Processor"""
        
        # ESG professional terminology dictionary
        self.esg_terms = {
            # Environmental indicators
            'environment': ['ç¯å¢ƒ', 'environment', 'ç¯ä¿', 'æ’æ”¾', 'emission', 'ç¢³', 'carbon'],
            'nitrogen_oxide': ['æ°®æ°§åŒ–ç‰©', 'nitrogen oxide', 'NOx', 'NO2'],
            'voc_emissions': ['VOCæ’æ”¾', 'VOC emissions', 'æŒ¥å‘æ€§æœ‰æœºåŒ–åˆç‰©', 'volatile organic compound'],
            'carbon_monoxide': ['ä¸€æ°§åŒ–ç¢³', 'carbon monoxide', 'CO'],
            'methane': ['ç”²çƒ·', 'methane', 'CH4'],
            'particulate': ['é¢—ç²’ç‰©', 'particulate', 'PM', 'ç²‰å°˜'],
            'energy_consumption': ['èƒ½æºæ¶ˆè€—', 'energy consumption', 'èƒ½è€—'],
            'renewable_energy': ['å¯å†ç”Ÿèƒ½æº', 'renewable energy', 'æ¸…æ´èƒ½æº'],
            'water_emissions': ['æ°´æ’æ”¾', 'water emissions', 'åºŸæ°´æ’æ”¾'],
            'hazardous_waste': ['å±é™©åºŸç‰©', 'hazardous waste', 'æœ‰å®³åºŸç‰©'],
            
            # Social indicators
            'social': ['ç¤¾ä¼š', 'social', 'ç¤¾ä¼šè´£ä»»', 'social responsibility'],
            'workforce': ['åŠ³åŠ¨åŠ›', 'workforce', 'å‘˜å·¥', 'employee', 'äººå‘˜'],
            'women_workforce': ['å¥³æ€§å‘˜å·¥', 'women workforce', 'å¥³æ€§åŠ³åŠ¨åŠ›', 'pct women', 'women percentage', 'Pct Women in Workforce'],
            'diversity': ['å¤šæ ·æ€§', 'diversity', 'å¤šå…ƒåŒ–'],
            'safety': ['å®‰å…¨', 'safety', 'èŒä¸šå®‰å…¨', 'occupational safety'],
            'training': ['åŸ¹è®­', 'training', 'æ•™è‚²', 'education'],
            'community': ['ç¤¾åŒº', 'community', 'ç¤¾åŒºå‚ä¸', 'community engagement'],
            'human_rights': ['äººæƒ', 'human rights', 'å‘˜å·¥æƒåˆ©', 'worker rights'],
            'indigenous_rights': ['åŸä½æ°‘æƒåˆ©', 'indigenous rights', 'åœŸè‘—æƒåˆ©'],
            'strikes': ['ç½¢å·¥', 'strikes', 'åŠ³èµ„çº çº·', 'labor disputes'],
            
            # Governance indicators
            'governance': ['æ²»ç†', 'governance', 'å…¬å¸æ²»ç†', 'corporate governance', 'Gç±»', 'Gç±»æŒ‡æ ‡', 'governanceæŒ‡æ ‡'],
            'board_diversity': ['è‘£äº‹ä¼šå¤šæ ·æ€§', 'board diversity', 'è‘£äº‹ä¼šå¤šå…ƒåŒ–'],
            'executive_compensation': ['é«˜ç®¡è–ªé…¬', 'executive compensation', 'ç®¡ç†å±‚è–ªé…¬'],
            'audit': ['å®¡è®¡', 'audit', 'å®¡è®¡è´¨é‡', 'audit quality'],
            'transparency': ['é€æ˜åº¦', 'transparency', 'ä¿¡æ¯æŠ«éœ²', 'disclosure'],
            'ethics': ['é“å¾·', 'ethics', 'å•†ä¸šé“å¾·', 'business ethics'],
            'compliance': ['åˆè§„', 'compliance', 'æ³•è§„éµå¾ª', 'regulatory compliance'],
            'risk_management': ['é£é™©ç®¡ç†', 'risk management', 'é£é™©æ§åˆ¶'],
            'stakeholder': ['åˆ©ç›Šç›¸å…³è€…', 'stakeholder', 'è‚¡ä¸œ', 'shareholder'],
            'sustainability': ['å¯æŒç»­æ€§', 'sustainability', 'å¯æŒç»­å‘å±•'],
            'financial_literacy': ['è´¢åŠ¡ç´ å…»', 'financial literacy', 'Financial Literacy Programs'],
            'management_diversity': ['ç®¡ç†å±‚å¤šæ ·æ€§', 'management diversity', 'Pct Minorities in Management'],
            
            # ES indicators
            'es_indicators': ['ESç±»', 'ESç±»æŒ‡æ ‡', 'ESæŒ‡æ ‡', 'ESç±»è¡¨ç°', 'ESè¡¨ç°'],
            'environmental_social': ['ç¯å¢ƒç¤¾ä¼š', 'environmental social', 'ES', 'E&S']
        }
        
        # Company name patterns (support ticker and company name)
        self.company_patterns = [
            # Ticker patterns (priority matching)
            r'([A-Z]{1,6}\s+US\s+Equity)',  # General ticker pattern (AA US Equity)
            r'(A US Equity|B US Equity|C US Equity|AA US Equity)',  # Specific ticker patterns
            # Company name patterns
            r'([A-Za-z\s]+(?:Inc|Corp|Ltd|Company|Technologies|Systems|Group|Holdings))',
            r'([A-Za-z\s]+(?:Inc\.|Corp\.|Ltd\.|Company\.))',
            r'([A-Za-z\s]+(?:Technologies|Systems|Group|Holdings))',
            # Mixed patterns
            r'([A-Za-z\s]+(?:Inc|Corp|Ltd|Company|Technologies|Systems|Group|Holdings)\s*\([A-Z]{1,6}\s+US\s+Equity\))',  # Company (Ticker)
            r'([A-Z]{1,6}\s+US\s+Equity\s*\([A-Za-z\s]+\))'  # Ticker (Company)
        ]
        
        # Year patterns
        self.year_patterns = [
            r'in\s+year\s+(\d{4})',  # in year 2006
            r'in\s+(\d{4})',        # in 2006
            r'year\s+(\d{4})',      # year 2006
            r'(\d{4})å¹´',
            r'(\d{4})',
            r'(\d{4})å¹´æ•°æ®',
            r'(\d{4})å¹´åº¦'
        ]
        
        # Indicator code patterns
        self.indicator_code_patterns = [
            r'ES\d{3}',
            r'ES\d{2}',
            r'code=ES\d{3}',
            r'æŒ‡æ ‡ä»£ç [ï¼š:]\s*ES\d{3}',
            r'ES\d{3}æŒ‡æ ‡'
        ]
        
        # Query intent classification
        self.query_intents = {
            'trend': ['è¶‹åŠ¿', 'trend', 'å˜åŒ–', 'change', 'å‘å±•', 'development', 'æ¼”å˜', 'evolution'],
            'comparison': ['æ¯”è¾ƒ', 'compare', 'å¯¹æ¯”', 'å¯¹æ¯”åˆ†æ', 'comparative', 'vs', 'versus'],
            'specific': ['å…·ä½“', 'specific', 'è¯¦ç»†', 'detail', 'å…·ä½“æ•°æ®', 'specific data'],
            'overview': ['æ¦‚è§ˆ', 'overview', 'æ€»ä½“', 'overall', 'æ•´ä½“', 'general', 'ç»¼åˆ'],
            'analysis': ['åˆ†æ', 'analysis', 'ç ”ç©¶', 'research', 'è¯„ä¼°', 'evaluation']
        }
    
    def process_query(self, raw_query: str, context: Dict = None) -> Dict[str, Any]:
        """Process raw query and return structured information"""
        # 1. Basic cleaning
        cleaned_query = self._clean_query(raw_query)
        
        # 2. Extract key information
        extracted_info = self._extract_key_information(cleaned_query)
        
        # 3. Handle context (e.g., "what about 2016?")
        if context and 'previous_query' in context:
            extracted_info = self._handle_context(extracted_info, context['previous_query'])
        
        # 4. Identify query intent
        intent = self._identify_intent(cleaned_query)
        
        # 5. Generate optimized query
        optimized_query = self._generate_optimized_query(extracted_info, intent)
        
        return {
            'raw_query': raw_query,
            'cleaned_query': cleaned_query,
            'extracted_info': extracted_info,
            'intent': intent,
            'optimized_query': optimized_query,
            'confidence': self._calculate_confidence(extracted_info),
            'timestamp': datetime.now().isoformat()
        }
    
    def _handle_context(self, extracted_info: Dict, previous_query: str) -> Dict:
        """Handle contextual queries"""
        # If current query lacks company info, get from previous query
        if not extracted_info['companies']:
            prev_companies = re.findall(r'([A-Za-z\s]+(?:Inc|Corp|Ltd|Company|Technologies|Systems|Group|Holdings|US Equity))', previous_query)
            if prev_companies:
                extracted_info['companies'] = prev_companies
        
        # If current query lacks indicator info, get from previous query
        if not extracted_info['indicators'] and not extracted_info['indicator_codes']:
            prev_indicators = re.findall(r'([A-Za-z\s]+(?:Emissions|Consumption|Policy|Rights|Workforce|Diversity))', previous_query)
            if prev_indicators:
                extracted_info['indicators'] = prev_indicators
        
        return extracted_info
    
    def _clean_query(self, query: str) -> str:
        """Clean query text"""
        # Remove extra spaces
        query = re.sub(r'\s+', ' ', query.strip())
        
        # Standardize punctuation
        query = query.replace('ï¼š', ':').replace('ï¼Œ', ',').replace('ã€‚', '.')
        
        # Remove special characters but keep important symbols
        query = re.sub(r'[^\w\s:,.()%-]', ' ', query)
        
        # Clean spaces again
        query = re.sub(r'\s+', ' ', query.strip())
        
        return query
    
    def _extract_key_information(self, query: str) -> Dict[str, Any]:
        """æå–å…³é”®ä¿¡æ¯"""
        info = {
            'companies': [],
            'years': [],
            'indicators': [],
            'indicator_codes': [],
            'values': [],
            'esg_categories': [],
            'keywords': []
        }
        
        # æ™ºèƒ½è§£ææŸ¥è¯¢ç»“æ„
        query_lower = query.lower()
        
        # 1. æå–å…¬å¸ä¿¡æ¯ï¼ˆä¼˜å…ˆåŒ¹é…tickerï¼‰
        for pattern in self.company_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match[0] else match[1]
                if match.strip() and len(match.strip()) > 1:
                    info['companies'].append(match.strip())
        
        # 2. æå–å¹´ä»½ä¿¡æ¯
        for pattern in self.year_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match[0] else match[1]
                if match.isdigit() and 1900 <= int(match) <= 2030:
                    info['years'].append(match)
        
        # 3. æå–æŒ‡æ ‡ä»£ç 
        for pattern in self.indicator_code_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            info['indicator_codes'].extend(matches)
        
        # 4. æ™ºèƒ½è¯†åˆ«æŒ‡æ ‡åç§°
        indicators = self._extract_indicator_names(query)
        info['indicators'].extend(indicators)
        
        # 5. è¯†åˆ«ESGç±»åˆ«
        for category, terms in self.esg_terms.items():
            for term in terms:
                if term.lower() in query_lower:
                    info['esg_categories'].append(category)
                    info['keywords'].append(term)
        
        # å»é‡å¹¶è¿‡æ»¤
        for key in ['companies', 'years', 'indicators', 'indicator_codes', 'values', 'esg_categories', 'keywords']:
            info[key] = list(set([item for item in info[key] if item and str(item).strip()]))
        
        return info
    
    def _extract_indicator_names(self, query: str) -> List[str]:
        """æ™ºèƒ½æå–æŒ‡æ ‡åç§°"""
        indicators = []
        query_lower = query.lower()
        
        # ç¯å¢ƒæŒ‡æ ‡
        env_patterns = [
            r'(nitrogen\s+oxide\s+emissions?)',
            r'(carbon\s+dioxide\s+emissions?)',
            r'(methane\s+emissions?)',
            r'(voc\s+emissions?)',
            r'(particulate\s+matter)',
            r'(water\s+emissions?)',
            r'(energy\s+consumption)',
            r'(renewable\s+energy)',
            r'(hazardous\s+waste)'
        ]
        
        # ç¤¾ä¼šæŒ‡æ ‡
        social_patterns = [
            r'(women\s+workforce)',
            r'(pct\s+women\s+in\s+workforce)',
            r'(employee\s+diversity)',
            r'(workforce\s+diversity)',
            r'(safety\s+training)',
            r'(community\s+engagement)',
            r'(human\s+rights)',
            r'(labor\s+rights)'
        ]
        
        # æ²»ç†æŒ‡æ ‡
        gov_patterns = [
            r'(board\s+diversity)',
            r'(executive\s+compensation)',
            r'(audit\s+quality)',
            r'(transparency)',
            r'(corporate\s+governance)',
            r'(risk\s+management)',
            r'(stakeholder\s+engagement)'
        ]
        
        all_patterns = env_patterns + social_patterns + gov_patterns
        
        for pattern in all_patterns:
            matches = re.findall(pattern, query_lower)
            indicators.extend(matches)
        
        return indicators
    
    def _identify_intent(self, query: str) -> str:
        """è¯†åˆ«æŸ¥è¯¢æ„å›¾"""
        query_lower = query.lower()
        
        for intent, keywords in self.query_intents.items():
            for keyword in keywords:
                if keyword.lower() in query_lower:
                    return intent
        
        return 'general'
    
    def _generate_optimized_query(self, extracted_info: Dict[str, Any], intent: str) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŸ¥è¯¢"""
        query_parts = []
        
        # æ·»åŠ ESGå…³é”®è¯
        if not extracted_info['esg_categories']:
            query_parts.append('ESG')
        
        # æ·»åŠ å…¬å¸ä¿¡æ¯ï¼ˆæ”¯æŒtickerå’Œcompany nameæŸ¥è¯¢ï¼‰
        if extracted_info['companies']:
            for company in extracted_info['companies']:
                company_clean = company.strip()
                # ç‰¹æ®Šå¤„ç†ï¼šA US Equity -> Agilent Technologies Inc
                if company_clean.lower() == 'a us equity':
                    query_parts.extend(['Agilent Technologies Inc', 'A US Equity'])
                elif company_clean.lower() == 'aa us equity':
                    query_parts.extend(['AA US Equity', 'Alcoa Corporation'])
                else:
                    query_parts.append(company_clean)
                    # å¦‚æœæ˜¯tickeræ ¼å¼ï¼Œæ·»åŠ å…¬å¸å…¨å
                    if 'us equity' in company_clean.lower():
                        ticker = company_clean.split()[0]
                        if ticker == 'AA':
                            query_parts.append('Alcoa Corporation')
                        elif ticker == 'A':
                            query_parts.append('Agilent Technologies Inc')
        
        # æ·»åŠ å¹´ä»½ä¿¡æ¯
        if extracted_info['years']:
            query_parts.extend(extracted_info['years'])
        
        # æ·»åŠ æŒ‡æ ‡ä¿¡æ¯
        if extracted_info['indicators']:
            query_parts.extend(extracted_info['indicators'])
        
        # æ·»åŠ æŒ‡æ ‡ä»£ç 
        if extracted_info['indicator_codes']:
            query_parts.extend(extracted_info['indicator_codes'])
        
        # æ·»åŠ å…³é”®è¯
        if extracted_info['keywords']:
            query_parts.extend(extracted_info['keywords'])
        
        # æ·»åŠ æ„å›¾ç›¸å…³è¯æ±‡
        if intent == 'trend':
            query_parts.append('è¶‹åŠ¿åˆ†æ')
        elif intent == 'comparison':
            query_parts.append('æ¯”è¾ƒåˆ†æ')
        elif intent == 'specific':
            query_parts.append('å…·ä½“æ•°æ®')
        elif intent == 'overview':
            query_parts.append('æ¦‚è§ˆ')
        elif intent == 'analysis':
            query_parts.append('åˆ†æ')
        
        # ç”Ÿæˆæœ€ç»ˆæŸ¥è¯¢
        optimized_query = ' '.join(query_parts)
        
        # æ·»åŠ ä¸­è‹±æ–‡æ··åˆä¼˜åŒ–
        if any(char in optimized_query for char in 'å…¬å¸å¹´æŒ‡æ ‡'):
            optimized_query += ' company year indicator'
        
        # ç‰¹æ®Šå¤„ç†Gç±»å’ŒESç±»æŒ‡æ ‡
        if 'Gç±»' in optimized_query or 'governance' in optimized_query.lower():
            optimized_query += ' governance corporate governance'
        if 'ESç±»' in optimized_query or 'ESæŒ‡æ ‡' in optimized_query:
            optimized_query += ' environmental social ES indicators'
        
        return optimized_query
    
    def _calculate_confidence(self, extracted_info: Dict[str, Any]) -> float:
        """è®¡ç®—æŸ¥è¯¢ç½®ä¿¡åº¦"""
        confidence = 0.0
        
        # åŸºç¡€åˆ†æ•°
        if extracted_info['companies']:
            confidence += 0.3
        if extracted_info['years']:
            confidence += 0.2
        if extracted_info['indicators'] or extracted_info['indicator_codes']:
            confidence += 0.3
        if extracted_info['esg_categories']:
            confidence += 0.2
        
        return min(confidence, 1.0)

class FixedMemoRAG:
    """Fixed MemoRAG System"""
    
    def __init__(self, db_path: str, model_name: str = "BAAI/bge-m3", 
                 memory_size: int = 1000, llm_api_key: str = None, language: str = "en"):
        """Initialize Fixed MemoRAG System"""
        self.db_path = db_path
        self.model_name = model_name
        self.memory_size = memory_size
        self.language = language  # "en" for English, "zh" for Chinese
        
        # Initialize translation dictionary
        self.translations = self._init_translations()
        
        # Initialize Chroma client
        self.client = chromadb.PersistentClient(path=db_path)
        
        # Load BGE model
        print(f"ğŸ”¬ {self.t('loading_model')}: {model_name}")
        self.model = SentenceTransformer(model_name)
        print(f"âœ… {self.t('model_loaded')}")
        
        # Check embedding dimensions
        test_embedding = self.model.encode(["test"])
        print(f"ğŸ“ {self.t('embedding_dimension')}: {len(test_embedding[0])}")
        
        # Check Chroma database embedding dimensions
        try:
            # Get all collections
            collections = self.client.list_collections()
            if collections:
                # Use first collection to check dimensions
                sample = self.client.get_collection(name=collections[0].name).get(limit=1, include=['embeddings'])
                if sample['embeddings']:
                    chroma_dim = len(sample['embeddings'][0])
                    print(f"ğŸ“ {self.t('chroma_dimension')}: {chroma_dim}")
                    if chroma_dim != len(test_embedding[0]):
                        print(f"âš ï¸ {self.t('dimension_mismatch')} BGE: {len(test_embedding[0])}, Chroma: {chroma_dim}")
                        print("ğŸ’¡ This may cause inaccurate similarity search")
                        print("ğŸ”§ Suggestion: Rebuild embeddings using the same model")
                    else:
                        print(f"âœ… {self.t('dimension_match')}")
                else:
                    print("âŒ Chroma database has no embedding data")
            else:
                print(f"âŒ {self.t('no_collections')}")
        except Exception as e:
            print(f"âŒ Failed to check Chroma embedding dimensions: {str(e)}")
        
        # Initialize LLM response generator
        if llm_api_key:
            self.llm_generator = LLMResponseGenerator(llm_api_key)
            print(f"ğŸ¤– {self.t('llm_initialized')}")
        else:
            self.llm_generator = None
            print(f"âš ï¸ {self.t('no_llm_config')}")
        
        # Initialize smart query processor
        self.query_processor = OptimizedQueryProcessor()
        print(f"ğŸ§  {self.t('query_processor_ready')}")
        
        # Initialize memory system
        self.memory = {
            'queries': [],
            'results': [],
            'patterns': {},
            'insights': [],
            'trends': {},
            'last_update': None
        }
        
        # Get all collections
        self.collections = self.client.list_collections()
        print(f"ğŸ“Š Found {len(self.collections)} {self.t('collections_found')}")
        
        # Display collection information
        if self.collections:
            print("ğŸ“‹ Available collections:")
            for i, collection in enumerate(self.collections, 1):
                try:
                    count = collection.count()
                    print(f"   {i}. {collection.name} (Records: {count})")
                except Exception as e:
                    print(f"   {i}. {collection.name} (Failed to get record count: {str(e)})")
        else:
            print(f"âš ï¸ {self.t('no_collections')}")
        
        # Load memory
        self.load_memory()
        
        # Context management
        self.last_query = None
        self.last_results = None
        
        # Answer mode management
        self.use_llm = llm_api_key is not None
        self.debug_mode = False
    
    def _init_translations(self) -> Dict[str, Dict[str, str]]:
        """Initialize translation dictionary"""
        return {
            # System messages
            "loading_model": {"en": "Loading BGE model", "zh": "æ­£åœ¨åŠ è½½BGEæ¨¡å‹"},
            "model_loaded": {"en": "BGE model loaded successfully!", "zh": "BGEæ¨¡å‹åŠ è½½å®Œæˆï¼"},
            "embedding_dimension": {"en": "BGE model embedding dimension", "zh": "BGEæ¨¡å‹åµŒå…¥ç»´åº¦"},
            "chroma_dimension": {"en": "Chroma database embedding dimension", "zh": "Chromaæ•°æ®åº“åµŒå…¥ç»´åº¦"},
            "dimension_match": {"en": "Embedding dimensions match", "zh": "åµŒå…¥ç»´åº¦åŒ¹é…"},
            "dimension_mismatch": {"en": "Warning: Embedding dimension mismatch!", "zh": "è­¦å‘Šï¼šåµŒå…¥ç»´åº¦ä¸åŒ¹é…ï¼"},
            "llm_initialized": {"en": "LLM response generator initialized!", "zh": "LLMå›ç­”ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼"},
            "no_llm_config": {"en": "No LLM API configured, using basic response mode", "zh": "æœªé…ç½®LLM APIï¼Œå°†ä½¿ç”¨åŸºç¡€å›ç­”æ¨¡å¼"},
            "query_processor_ready": {"en": "Smart query processor initialized!", "zh": "æ™ºèƒ½æŸ¥è¯¢å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼"},
            "collections_found": {"en": "ESG data collections found", "zh": "ä¸ªESGæ•°æ®é›†åˆ"},
            "no_collections": {"en": "Warning: No data collections found", "zh": "è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®é›†åˆ"},
            "memory_loaded": {"en": "Memory loaded successfully", "zh": "åŠ è½½è®°å¿†æˆåŠŸ"},
            "new_memory": {"en": "Creating new memory bank", "zh": "åˆ›å»ºæ–°çš„è®°å¿†åº“"},
            
            # Query processing
            "auto_detected_collection": {"en": "Auto-detected collection", "zh": "è‡ªåŠ¨æ£€æµ‹åˆ°é›†åˆ"},
            "no_collections_found": {"en": "No collections found", "zh": "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é›†åˆ"},
            "extracted_companies": {"en": "Extracted companies", "zh": "æå–åˆ°çš„å…¬å¸"},
            "no_company_info": {"en": "No company information extracted", "zh": "æœªæå–åˆ°å…¬å¸ä¿¡æ¯"},
            
            # Results display
            "query": {"en": "Query", "zh": "æŸ¥è¯¢"},
            "smart_response": {"en": "Smart Response", "zh": "æ™ºèƒ½å›ç­”"},
            "data_summary": {"en": "Data Summary", "zh": "æ•°æ®æ‘˜è¦"},
            "found_results": {"en": "relevant data found (sorted by quality)", "zh": "æ¡ç›¸å…³æ•°æ®ï¼ˆå·²æŒ‰è´¨é‡é‡æ’åºï¼‰"},
            "no_results": {"en": "No relevant results found", "zh": "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ"},
            "more_data": {"en": "more data", "zh": "æ¡æ•°æ®"},
            
            # Commands
            "help": {"en": "help", "zh": "å¸®åŠ©"},
            "collections": {"en": "collections", "zh": "é›†åˆ"},
            "memory": {"en": "memory", "zh": "è®°å¿†"},
            "clear": {"en": "clear", "zh": "æ¸…ç©º"},
            "mode": {"en": "mode", "zh": "æ¨¡å¼"},
            "debug": {"en": "debug", "zh": "è°ƒè¯•"},
            "quit": {"en": "quit/exit", "zh": "é€€å‡º"},
            
            # Interactive mode
            "welcome": {"en": "Welcome to Smart MemoRAG ESG System!", "zh": "æ¬¢è¿ä½¿ç”¨æ™ºèƒ½å›ç­”ç‰ˆMemoRAG ESGç³»ç»Ÿï¼"},
            "enter_query": {"en": "Enter your ESG query, or type 'help' for help", "zh": "è¾“å…¥æ‚¨çš„ESGæŸ¥è¯¢ï¼Œæˆ–è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©"},
            "enter_query_prompt": {"en": "Please enter your query", "zh": "è¯·è¾“å…¥æ‚¨çš„æŸ¥è¯¢"},
            "invalid_query": {"en": "Please enter a valid query", "zh": "è¯·è¾“å…¥æœ‰æ•ˆçš„æŸ¥è¯¢"},
            "processing_error": {"en": "Error processing query", "zh": "å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™"},
            "try_again": {"en": "Please try entering the query again", "zh": "è¯·å°è¯•é‡æ–°è¾“å…¥æŸ¥è¯¢"},
            "goodbye": {"en": "Thank you for using! Goodbye!", "zh": "æ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼"},
            
            # Help system
            "usage_help": {"en": "Usage Help", "zh": "ä½¿ç”¨å¸®åŠ©"},
            "query_examples": {"en": "Query Examples", "zh": "æŸ¥è¯¢ç¤ºä¾‹"},
            "commands": {"en": "Commands", "zh": "å‘½ä»¤"},
            "tips": {"en": "Tips", "zh": "æç¤º"},
            "supports_natural_language": {"en": "Supports natural language queries", "zh": "æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢"},
            "supports_multilingual": {"en": "Supports multilingual queries", "zh": "æ”¯æŒä¸­è‹±æ–‡æ··åˆæŸ¥è¯¢"},
            "supports_context": {"en": "Supports contextual queries", "zh": "æ”¯æŒä¸Šä¸‹æ–‡æŸ¥è¯¢"},
            "generates_smart_responses": {"en": "Generates smart responses", "zh": "ç³»ç»Ÿä¼šç”Ÿæˆæ™ºèƒ½å›ç­”"},
            "remembers_history": {"en": "Remembers your query history", "zh": "ç³»ç»Ÿä¼šè®°ä½æ‚¨çš„æŸ¥è¯¢å†å²"},
            "auto_detects_collections": {"en": "Auto-detects available data collections", "zh": "ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„æ•°æ®é›†åˆ"},
            
            # Memory system
            "memory_empty": {"en": "Memory bank is empty", "zh": "è®°å¿†åº“ä¸ºç©º"},
            "memory_report": {"en": "Memory Report", "zh": "è®°å¿†æŠ¥å‘Š"},
            "total_queries": {"en": "Total queries", "zh": "æ€»æŸ¥è¯¢æ•°"},
            "recent_queries": {"en": "Recent queries", "zh": "æœ€è¿‘æŸ¥è¯¢"},
            "popular_companies": {"en": "Popular companies", "zh": "çƒ­é—¨å…¬å¸"},
            "popular_years": {"en": "Popular years", "zh": "çƒ­é—¨å¹´ä»½"},
            "pattern_count": {"en": "Pattern count", "zh": "æ¨¡å¼æ•°é‡"},
            "trend_count": {"en": "Trend count", "zh": "è¶‹åŠ¿æ•°é‡"},
            "memory_cleared": {"en": "Memory cleared", "zh": "è®°å¿†å·²æ¸…ç©º"},
            
            # Collections
            "available_collections": {"en": "Available Data Collections", "zh": "å¯ç”¨æ•°æ®é›†åˆ"},
            "collection_name": {"en": "Collection Name", "zh": "é›†åˆåç§°"},
            "record_count": {"en": "Record Count", "zh": "è®°å½•æ•°é‡"},
            "sample_metadata": {"en": "Sample Metadata", "zh": "æ ·æœ¬å…ƒæ•°æ®"},
            "failed_to_get_info": {"en": "Failed to get information", "zh": "è·å–ä¿¡æ¯å¤±è´¥"},
            
            # Debug mode
            "debug_mode_on": {"en": "Debug mode enabled", "zh": "è°ƒè¯•æ¨¡å¼å·²å¼€å¯"},
            "debug_mode_off": {"en": "Debug mode disabled", "zh": "è°ƒè¯•æ¨¡å¼å·²å…³é—­"},
            "debug_info": {"en": "Debug Information", "zh": "è°ƒè¯•ä¿¡æ¯"},
            "raw_query": {"en": "Raw Query", "zh": "åŸå§‹æŸ¥è¯¢"},
            "cleaned_query": {"en": "Cleaned Query", "zh": "æ¸…ç†åæŸ¥è¯¢"},
            "optimized_query": {"en": "Optimized Query", "zh": "ä¼˜åŒ–æŸ¥è¯¢"},
            "extracted_info": {"en": "Extracted Information", "zh": "æå–ä¿¡æ¯"},
            "query_intent": {"en": "Query Intent", "zh": "æŸ¥è¯¢æ„å›¾"},
            "confidence": {"en": "Confidence", "zh": "ç½®ä¿¡åº¦"},
            "original_document": {"en": "Original Document", "zh": "åŸå§‹æ–‡æ¡£"},
            "metadata": {"en": "Metadata", "zh": "å…ƒæ•°æ®"},
            "extraction_result": {"en": "Extraction Result", "zh": "æå–ç»“æœ"},
            
            # LLM mode
            "llm_mode": {"en": "LLM Smart Response", "zh": "LLMæ™ºèƒ½å›ç­”"},
            "basic_mode": {"en": "Basic Response", "zh": "åŸºç¡€å›ç­”"},
            "switched_to_mode": {"en": "Switched to", "zh": "å·²åˆ‡æ¢åˆ°"},
            "no_llm_api": {"en": "No LLM API configured, cannot switch modes", "zh": "æœªé…ç½®LLM APIï¼Œæ— æ³•åˆ‡æ¢æ¨¡å¼"},
            
            # Main function
            "system_title": {"en": "Smart MemoRAG + BGE-M3 ESG System", "zh": "æ™ºèƒ½å›ç­”ç‰ˆMemoRAG + BGE-M3 ESGç³»ç»Ÿ"},
            "use_llm_prompt": {"en": "Use LLM to generate smart responses? (y/n)", "zh": "æ˜¯å¦ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½å›ç­”ï¼Ÿ(y/n)"},
            "api_key_prompt": {"en": "Enter DeepSeek API Key", "zh": "è¯·è¾“å…¥DeepSeek API Key"},
            "no_api_key": {"en": "No API Key provided, using basic response mode", "zh": "æœªæä¾›API Keyï¼Œå°†ä½¿ç”¨åŸºç¡€å›ç­”æ¨¡å¼"},
            
            # Language selection
            "language_selection": {"en": "Language Selection", "zh": "è¯­è¨€é€‰æ‹©"},
            "select_language": {"en": "Please select language / è¯·é€‰æ‹©è¯­è¨€:", "zh": "è¯·é€‰æ‹©è¯­è¨€ / Please select language:"},
            "english_option": {"en": "1. English", "zh": "1. English"},
            "chinese_option": {"en": "2. Chinese (ä¸­æ–‡)", "zh": "2. Chinese (ä¸­æ–‡)"},
            "language_set": {"en": "Language set to", "zh": "è¯­è¨€å·²è®¾ç½®ä¸º"},
            
            # Search results display
            "searched_companies": {"en": "Searched companies", "zh": "æœç´¢åˆ°çš„å…¬å¸"},
            "unique_companies_found": {"en": "Unique companies found", "zh": "å‘ç°çš„å”¯ä¸€å…¬å¸"},
            "similarity": {"en": "Similarity", "zh": "ç›¸ä¼¼åº¦"},
            "quality_score": {"en": "Quality score", "zh": "åˆ†æ•°"},
            "more_data_available": {"en": "more data available", "zh": "æ¡æ•°æ®"},
        }
    
    def t(self, key: str) -> str:
        """Get translated text"""
        if key in self.translations:
            return self.translations[key].get(self.language, self.translations[key]["en"])
        return key
    
    def intelligent_query(self, raw_query: str, collection_name: str = None, 
                        n_results: int = 10, use_memory: bool = True) -> Dict[str, Any]:
        """Intelligent query (supports natural language)"""
        # Auto-detect collection name
        if collection_name is None:
            collections = self.client.list_collections()
            if collections:
                collection_name = collections[0].name
                print(f"ğŸ” {self.t('auto_detected_collection')}: {collection_name}")
            else:
                print(f"âŒ {self.t('no_collections_found')}")
                return {
                    'raw_query': raw_query,
                    'query_analysis': {},
                    'results': [],
                    'insights': [self.t('no_collections_found')],
                    'smart_response': f"Sorry, no data collections found in the database.",
                    'timestamp': datetime.now().isoformat()
                }
        
        # 1. Process query (with context)
        context = {'previous_query': self.last_query} if self.last_query else None
        query_analysis = self.query_processor.process_query(raw_query, context)
        
        # Debug mode: display query analysis
        if self.debug_mode:
            print(f"\nğŸ” {self.t('debug_info')}:")
            print(f"   {self.t('raw_query')}: {raw_query}")
            print(f"   {self.t('cleaned_query')}: {query_analysis['cleaned_query']}")
            print(f"   {self.t('optimized_query')}: {query_analysis['optimized_query']}")
            print(f"   {self.t('extracted_info')}: {query_analysis['extracted_info']}")
            print(f"   {self.t('query_intent')}: {query_analysis['intent']}")
            print(f"   {self.t('confidence')}: {query_analysis['confidence']:.2f}")
        
        # Always display extracted company info (for debugging)
        extracted_companies = query_analysis['extracted_info'].get('companies', [])
        if extracted_companies:
            print(f"\nğŸ¢ {self.t('extracted_companies')}: {extracted_companies}")
        else:
            print(f"\nâš ï¸ {self.t('no_company_info')}")
        
        # 2. Update context
        self.last_query = raw_query
        
        # 3. Execute query
        results = self._execute_query_with_post_filter(query_analysis, collection_name, n_results)
        
        # 4. Generate insights
        insights = self._generate_insights(results, query_analysis)
        
        # 5. Generate smart response
        smart_response = self._generate_smart_response(raw_query, results, query_analysis)
        
        # 6. Add to memory
        self.add_to_memory(query_analysis['optimized_query'], results, insights)
        
        # 7. Update context results
        self.last_results = results
        
        return {
            'raw_query': raw_query,
            'query_analysis': query_analysis,
            'results': results,
            'insights': insights,
            'smart_response': smart_response,
            'timestamp': datetime.now().isoformat()
        }
    
    def _generate_smart_response(self, query: str, results: List[Dict], query_analysis: Dict) -> str:
        """Generate smart response"""
        if self.use_llm and self.llm_generator:
            # Use LLM to generate response
            context = f"Query intent: {query_analysis['intent']}, Confidence: {query_analysis['confidence']:.2f}"
            # Pass language setting to LLM generator
            self.llm_generator.language = self.language
            return self.llm_generator.generate_response(query, results, context)
        else:
            # Use basic response generation
            return self._generate_basic_response(query, results, query_analysis)
    
    def _generate_basic_response(self, query: str, results: List[Dict], query_analysis: Dict) -> str:
        """Generate basic response"""
        if not results:
            if self.language == 'zh':
                return "å¾ˆæŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨æŸ¥è¯¢ç›¸å…³çš„æ•°æ®ã€‚è¯·å°è¯•è°ƒæ•´æŸ¥è¯¢æ¡ä»¶æˆ–æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨ã€‚"
            else:
                return "Sorry, no data related to your query was found. Please try adjusting your query conditions or check if the data exists."
        
        # Analyze query intent
        intent = query_analysis['intent']
        extracted_info = query_analysis['extracted_info']
        
        response_parts = []
        
        # Opening
        if self.language == 'zh':
            if intent == 'trend':
                response_parts.append("æ ¹æ®è¶‹åŠ¿åˆ†æï¼Œ")
            elif intent == 'comparison':
                response_parts.append("é€šè¿‡å¯¹æ¯”åˆ†æï¼Œ")
            elif intent == 'specific':
                response_parts.append("å…·ä½“æ•°æ®æ˜¾ç¤ºï¼Œ")
            else:
                response_parts.append("æ ¹æ®æŸ¥è¯¢ç»“æœï¼Œ")
        else:
            if intent == 'trend':
                response_parts.append("Based on trend analysis,")
            elif intent == 'comparison':
                response_parts.append("Through comparative analysis,")
            elif intent == 'specific':
                response_parts.append("Specific data shows,")
            else:
                response_parts.append("Based on query results,")
        
        # Data statistics
        companies = list(set([r['esg_info'].get('company', '') for r in results if r['esg_info'].get('company')]))
        years = list(set([r['esg_info'].get('year', '') for r in results if r['esg_info'].get('year')]))
        indicators = list(set([r['esg_info'].get('indicator', '') for r in results if r['esg_info'].get('indicator')]))
        
        if self.language == 'zh':
            response_parts.append(f"æˆ‘æ‰¾åˆ°äº† {len(results)} æ¡ç›¸å…³æ•°æ®ã€‚")
            
            if companies:
                response_parts.append(f"æ¶‰åŠçš„å…¬å¸åŒ…æ‹¬: {', '.join(companies[:3])}ã€‚")
            
            if years:
                response_parts.append(f"æ•°æ®å¹´ä»½èŒƒå›´: {min(years)}-{max(years)}ã€‚")
            
            if indicators:
                response_parts.append(f"ä¸»è¦æŒ‡æ ‡åŒ…æ‹¬: {', '.join(indicators[:3])}ã€‚")
            
            # Trend analysis
            if len(results) > 1 and years:
                response_parts.append("ä»æ—¶é—´ç»´åº¦çœ‹ï¼Œæ•°æ®å‘ˆç°ä¸€å®šçš„å˜åŒ–è¶‹åŠ¿ã€‚")
            
            # Suggestions
            response_parts.append("å»ºè®®æ‚¨æŸ¥çœ‹å…·ä½“æ•°æ®è¯¦æƒ…ä»¥è·å–æ›´å‡†ç¡®çš„ä¿¡æ¯ã€‚")
        else:
            response_parts.append(f"I found {len(results)} relevant data records.")
            
            if companies:
                response_parts.append(f"Companies involved include: {', '.join(companies[:3])}.")
            
            if years:
                response_parts.append(f"Data year range: {min(years)}-{max(years)}.")
            
            if indicators:
                response_parts.append(f"Main indicators include: {', '.join(indicators[:3])}.")
            
            # Trend analysis
            if len(results) > 1 and years:
                response_parts.append("From a temporal perspective, the data shows certain trends.")
            
            # Suggestions
            response_parts.append("I recommend reviewing specific data details for more accurate information.")
        
        return " ".join(response_parts)
    
    def toggle_llm_mode(self):
        """Toggle LLM mode"""
        if self.llm_generator and self.llm_generator.api_key:
            self.use_llm = not self.use_llm
            mode = self.t('llm_mode') if self.use_llm else self.t('basic_mode')
            print(f"ğŸ”„ {self.t('switched_to_mode')} {mode}")
        else:
            print(f"âŒ {self.t('no_llm_api')}")
    
    def _execute_query_with_post_filter(self, query_analysis: Dict, collection_name: str, n_results: int) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿›è¡Œåè¿‡æ»¤"""
        try:
            collection = self.client.get_collection(name=collection_name)
            query_embedding = self.model.encode([query_analysis['optimized_query']])[0].tolist()
            
            # å…ˆæ‰§è¡ŒåŸºç¡€æŸ¥è¯¢ï¼Œè·å–æ›´å¤šç»“æœ
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=min(50, n_results * 5),  # è·å–æ›´å¤šç»“æœç”¨äºè¿‡æ»¤
                include=['documents', 'metadatas', 'distances']
            )
            
            # å¤„ç†ç»“æœ
            processed_results = []
            for i in range(len(results['documents'][0])):
                document = results['documents'][0][i]
                metadata = results['metadatas'][0][i]
                distance = results['distances'][0][i]
                similarity = 1 - distance
                
                esg_info = self.extract_esg_info(document)
                
                result = {
                    'document': document,
                    'metadata': metadata,
                    'distance': distance,
                    'similarity': similarity,
                    'esg_info': esg_info
                }
                
                processed_results.append(result)
            
            # Debug: display all searched companies
            print(f"\nğŸ” {self.t('searched_companies')} ({len(processed_results)} records):")
            companies_found = set()
            for i, r in enumerate(processed_results[:10]):
                esg_info = r['esg_info']
                metadata = r['metadata']
                company = esg_info.get('company', metadata.get('company', 'N/A'))
                companies_found.add(company)
                print(f"   {i+1}. {company} ({self.t('similarity')}: {r['similarity']:.3f})")
                
                # Debug: display original document and metadata
                if self.debug_mode:
                    print(f"      ğŸ“„ {self.t('original_document')}: {r['document'][:100]}...")
                    print(f"      ğŸ“‹ {self.t('metadata')}: {metadata}")
                    print(f"      ğŸ” {self.t('extraction_result')}: {esg_info}")
                    print()
            
            print(f"\nğŸ“Š {self.t('unique_companies_found')}: {list(companies_found)}")
            
            # åè¿‡æ»¤
            filtered_results = self._post_filter_results(processed_results, query_analysis)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            filtered_results.sort(key=lambda x: x['similarity'], reverse=True)
            
            return filtered_results[:n_results]
            
        except Exception as e:
            print(f"Query error: {str(e)}")
            return []
    
    def _post_filter_results(self, results: List[Dict], query_analysis: Dict) -> List[Dict]:
        """åè¿‡æ»¤ç»“æœå¹¶è¿›è¡Œæ™ºèƒ½é‡æ’åº"""
        extracted_info = query_analysis['extracted_info']
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•è¿‡æ»¤æ¡ä»¶ï¼Œç›´æ¥è¿”å›åŸå§‹ç»“æœ
        if not extracted_info['years'] and not extracted_info['companies'] and not extracted_info['indicator_codes']:
            return self._rerank_results(results, query_analysis)
        
        # è®¡ç®—æ¯ä¸ªç»“æœçš„åŒ¹é…åˆ†æ•°
        scored_results = []
        for result in results:
            score = self._calculate_match_score(result, extracted_info)
            if score > 0:  # åªä¿ç•™æœ‰åŒ¹é…çš„ç»“æœ
                result['match_score'] = score
                scored_results.append(result)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„ç»“æœï¼Œè¿”å›åŸå§‹ç»“æœ
        if not scored_results:
            return self._rerank_results(results, query_analysis)
        
        # æ™ºèƒ½é‡æ’åº
        reranked_results = self._rerank_results(scored_results, query_analysis)
        
        return reranked_results
    
    def _calculate_match_score(self, result: Dict, extracted_info: Dict) -> int:
        """è®¡ç®—åŒ¹é…åˆ†æ•°"""
        metadata = result['metadata']
        esg_info = result['esg_info']
        score = 0
        
        # å¹´ä»½åŒ¹é…åˆ†æ•°
        if extracted_info['years']:
            for year in extracted_info['years']:
                if metadata.get('year') and str(metadata.get('year')) == str(year):
                    score += 10
                elif esg_info.get('year') and str(esg_info.get('year')) == str(year):
                    score += 10
        
        # å…¬å¸åŒ¹é…åˆ†æ•°ï¼ˆæ›´çµæ´»çš„åŒ¹é…ï¼‰
        if extracted_info['companies']:
            for company in extracted_info['companies']:
                # æ£€æŸ¥metadataä¸­çš„companyå­—æ®µ
                if metadata.get('company'):
                    metadata_company = str(metadata.get('company')).lower()
                    if company.lower() in metadata_company:
                        score += 10
                    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæŸ¥è¯¢çš„æ˜¯"A US Equity"ï¼Œä¹ŸåŒ¹é…åŒ…å«"Agilent"çš„è®°å½•
                    elif company.lower() == 'a us equity' and 'agilent' in metadata_company:
                        score += 10
                
                # æ£€æŸ¥esg_infoä¸­çš„companyå­—æ®µ
                if esg_info.get('company'):
                    esg_company = str(esg_info.get('company')).lower()
                    if company.lower() in esg_company:
                        score += 10
                    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæŸ¥è¯¢çš„æ˜¯"A US Equity"ï¼Œä¹ŸåŒ¹é…åŒ…å«"Agilent"çš„è®°å½•
                    elif company.lower() == 'a us equity' and 'agilent' in esg_company:
                        score += 10
        
        # æŒ‡æ ‡ä»£ç åŒ¹é…åˆ†æ•°
        if extracted_info['indicator_codes']:
            for code in extracted_info['indicator_codes']:
                if metadata.get('field_code') and code.upper() in str(metadata.get('field_code')).upper():
                    score += 10
                elif esg_info.get('code') and code.upper() in str(esg_info.get('code')).upper():
                    score += 10
        
        return score
    
    def _rerank_results(self, results: List[Dict], query_analysis: Dict) -> List[Dict]:
        """æ™ºèƒ½é‡æ’åºç»“æœ"""
        extracted_info = query_analysis['extracted_info']
        
        # ä¸ºæ¯ä¸ªç»“æœè®¡ç®—é‡æ’åºåˆ†æ•°
        for result in results:
            score = 0
            esg_info = result['esg_info']
            metadata = result['metadata']
            
            # 1. æ•°æ®å®Œæ•´æ€§åˆ†æ•°ï¼ˆæœ€é‡è¦ï¼‰
            value = esg_info.get('value', '')
            if value and str(value).lower() != 'nan' and str(value).strip():
                score += 100  # æœ‰æ•ˆæ•°æ®é«˜åˆ†
            else:
                score -= 20  # æ— æ•ˆæ•°æ®è½»å¾®æ‰£åˆ†ï¼ˆä¸è¦å®Œå…¨æ’é™¤ï¼‰
            
            # 2. åŒ¹é…åˆ†æ•°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if 'match_score' in result:
                score += result['match_score'] * 5
            
            # 3. å¹´ä»½åŒ¹é…åˆ†æ•°
            if extracted_info['years']:
                for year in extracted_info['years']:
                    if metadata.get('year') and str(metadata.get('year')) == str(year):
                        score += 50
                    elif esg_info.get('year') and str(esg_info.get('year')) == str(year):
                        score += 50
            
            # 4. å…¬å¸åŒ¹é…åˆ†æ•°ï¼ˆæ›´çµæ´»çš„åŒ¹é…ï¼‰
            if extracted_info['companies']:
                for company in extracted_info['companies']:
                    # æ£€æŸ¥metadataä¸­çš„companyå­—æ®µ
                    if metadata.get('company'):
                        metadata_company = str(metadata.get('company')).lower()
                        if company.lower() in metadata_company:
                            score += 30
                        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæŸ¥è¯¢çš„æ˜¯"A US Equity"ï¼Œä¹ŸåŒ¹é…åŒ…å«"Agilent"çš„è®°å½•
                        elif company.lower() == 'a us equity' and 'agilent' in metadata_company:
                            score += 30
                    
                    # æ£€æŸ¥esg_infoä¸­çš„companyå­—æ®µ
                    if esg_info.get('company'):
                        esg_company = str(esg_info.get('company')).lower()
                        if company.lower() in esg_company:
                            score += 30
                        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæŸ¥è¯¢çš„æ˜¯"A US Equity"ï¼Œä¹ŸåŒ¹é…åŒ…å«"Agilent"çš„è®°å½•
                        elif company.lower() == 'a us equity' and 'agilent' in esg_company:
                            score += 30
            
            # 5. æŒ‡æ ‡ä»£ç åŒ¹é…åˆ†æ•°
            if extracted_info['indicator_codes']:
                for code in extracted_info['indicator_codes']:
                    if metadata.get('field_code') and code.upper() in str(metadata.get('field_code')).upper():
                        score += 20
                    elif esg_info.get('code') and code.upper() in str(esg_info.get('code')).upper():
                        score += 20
            
            # 6. ç›¸ä¼¼åº¦åˆ†æ•°
            score += result.get('similarity', 0) * 10
            
            # 7. ESGç±»åˆ«åŒ¹é…åˆ†æ•°
            if extracted_info['esg_categories']:
                score += 10
            
            result['rerank_score'] = score
        
        # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
        results.sort(key=lambda x: x['rerank_score'], reverse=True)
        
        return results
    
    def extract_esg_info(self, document: str) -> Dict[str, str]:
        """ä»æ–‡æ¡£ä¸­æå–ESGä¿¡æ¯"""
        info = {}
        
        try:
            # Debug: print original document
            if hasattr(self, 'debug_mode') and self.debug_mode:
                print(f"ğŸ” {self.t('original_document')}: {document[:200]}...")
            
            # Extract company name - support multiple formats
            company_patterns = [
                r'([^ï¼ˆ]+)ï¼ˆ',  # Chinese format: company nameï¼ˆ
                r'([A-Za-z\s]+(?:Inc|Corp|Ltd|Company|Technologies|Systems|Group|Holdings))',  # English format
                r'([A-Z]{1,6}\s+US\s+Equity)',  # Ticker format
            ]
            
            for pattern in company_patterns:
                company_match = re.search(pattern, document)
                if company_match:
                    info['company'] = company_match.group(1).strip()
                    break
            
            # Extract year - support multiple formats
            year_patterns = [
                r'åœ¨(\d{4})å¹´',  # Chinese format: åœ¨2006å¹´
                r'(\d{4})',      # Simple number format
                r'year\s+(\d{4})',  # year 2006
                r'in\s+(\d{4})',   # in 2006
            ]
            
            for pattern in year_patterns:
                year_match = re.search(pattern, document, re.IGNORECASE)
                if year_match:
                    info['year'] = year_match.group(1)
                    break
            
            # æå–æŒ‡æ ‡å - æ”¯æŒå¤šç§æ ¼å¼
            indicator_patterns = [
                r'ï¼š([^ï¼ˆ]+)ï¼ˆ',  # ä¸­æ–‡æ ¼å¼ï¼šæŒ‡æ ‡åï¼ˆ
                r'([A-Za-z\s]+(?:Emissions|Consumption|Policy|Rights|Workforce|Diversity|Governance))',  # è‹±æ–‡æŒ‡æ ‡
                r'(nitrogen\s+oxide\s+emissions?)',  # å…·ä½“æŒ‡æ ‡
                r'(carbon\s+dioxide\s+emissions?)',
                r'(methane\s+emissions?)',
                r'(voc\s+emissions?)',
                r'(women\s+workforce)',
                r'(pct\s+women\s+in\s+workforce)',
            ]
            
            for pattern in indicator_patterns:
                indicator_match = re.search(pattern, document, re.IGNORECASE)
                if indicator_match:
                    info['indicator'] = indicator_match.group(1).strip()
                    break
            
            # æå–æŒ‡æ ‡ä»£ç 
            code_patterns = [
                r'code=([^ï¼‰]+)',  # code=ES001
                r'(ES\d{3})',      # ES001
                r'(ES\d{2})',     # ES01
            ]
            
            for pattern in code_patterns:
                code_match = re.search(pattern, document, re.IGNORECASE)
                if code_match:
                    info['code'] = code_match.group(1).strip()
                    break
            
            # æå–æ•°å€¼ - æ”¯æŒå¤šç§æ ¼å¼
            value_patterns = [
                r'= ([^,]+)',     # = 42.6
                r':\s*([^,\s]+)', # : 42.6
                r'(\d+\.?\d*)',   # ç®€å•æ•°å­—
                r'(True|False)',  # å¸ƒå°”å€¼
            ]
            
            for pattern in value_patterns:
                value_match = re.search(pattern, document)
                if value_match:
                    info['value'] = value_match.group(1).strip()
                    break
            
            # Debug: print extraction results
            if hasattr(self, 'debug_mode') and self.debug_mode:
                print(f"ğŸ” {self.t('extraction_result')}: {info}")
            
        except Exception as e:
            print(f"Error extracting information: {str(e)}")
        
        return info
    
    def _generate_insights(self, results: List[Dict], query_analysis: Dict) -> List[str]:
        """ç”Ÿæˆæ´å¯Ÿ"""
        insights = []
        
        if not results:
            return ["æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ•°æ®"]
        
        # åŸºäºç›¸ä¼¼åº¦åˆ†æ
        high_similarity_count = len([r for r in results if r['similarity'] > 0.8])
        if high_similarity_count > 0:
            insights.append(f"å‘ç° {high_similarity_count} æ¡é«˜ç›¸å…³æ€§æ•°æ®ï¼ˆç›¸ä¼¼åº¦>0.8ï¼‰")
        
        # åŸºäºæ•°æ®å®Œæ•´æ€§åˆ†æ
        complete_data = [r for r in results if not r['metadata'].get('incomplete', False)]
        if complete_data:
            insights.append(f"æ•°æ®å®Œæ•´æ€§è‰¯å¥½ï¼Œ{len(complete_data)} æ¡è®°å½•å®Œæ•´")
        
        # åŸºäºå…¬å¸åˆ†æ
        companies = [r['esg_info'].get('company', '') for r in results if r['esg_info'].get('company')]
        if companies:
            unique_companies = list(set(companies))
            insights.append(f"æ¶‰åŠ {len(unique_companies)} å®¶å…¬å¸: {', '.join(unique_companies[:3])}")
        
        # åŸºäºå¹´ä»½åˆ†æ
        years = [r['esg_info'].get('year') for r in results if r['esg_info'].get('year')]
        if years:
            unique_years = sorted(list(set(years)))
            insights.append(f"æ•°æ®å¹´ä»½èŒƒå›´: {min(unique_years)}-{max(unique_years)}")
        
        return insights
    
    def add_to_memory(self, query: str, results: List[Dict], insights: List[str] = None):
        """æ·»åŠ æŸ¥è¯¢åˆ°è®°å¿†ç³»ç»Ÿ"""
        memory_entry = {
            'timestamp': datetime.now().isoformat(),
            'query': query,
            'result_count': len(results),
            'top_similarity': max([r.get('similarity', 0) for r in results]) if results else 0,
            'insights': insights or [],
            'companies': list(set([r.get('metadata', {}).get('company', '') for r in results])),
            'years': list(set([r.get('metadata', {}).get('year') for r in results if r.get('metadata', {}).get('year')])),
            'indicators': list(set([r.get('metadata', {}).get('field_name', '') for r in results]))
        }
        
        self.memory['queries'].append(memory_entry)
        
        # ä¿æŒè®°å¿†å¤§å°
        if len(self.memory['queries']) > self.memory_size:
            self.memory['queries'] = self.memory['queries'][-self.memory_size:]
        
        # ä¿å­˜è®°å¿†
        self.save_memory()
    
    def save_memory(self):
        """ä¿å­˜è®°å¿†åˆ°æ–‡ä»¶"""
        memory_file = os.path.join(self.db_path, 'memorag_memory.pkl')
        try:
            with open(memory_file, 'wb') as f:
                pickle.dump(self.memory, f)
        except Exception as e:
            print(f"Failed to save memory: {str(e)}")
    
    def load_memory(self):
        """ä»æ–‡ä»¶åŠ è½½è®°å¿†"""
        memory_file = os.path.join(self.db_path, 'memorag_memory.pkl')
        if os.path.exists(memory_file):
            try:
                with open(memory_file, 'rb') as f:
                    self.memory = pickle.load(f)
                print(f"âœ… {self.t('memory_loaded')}, contains {len(self.memory['queries'])} historical queries")
            except Exception as e:
                print(f"Failed to load memory: {str(e)}")
        else:
            print(f"ğŸ“ {self.t('new_memory')}")
    
    def display_results(self, result: Dict[str, Any]):
        """Display query results (smart response version)"""
        print(f"\nğŸ” {self.t('query')}: {result['raw_query']}")
        print("=" * 60)
        
        # Display smart response
        print(f"ğŸ¤– {self.t('smart_response')}:")
        print(result['smart_response'])
        
        # Display data summary
        results = result['results']
        if results:
            print(f"\nğŸ“Š {self.t('data_summary')}:")
            print(f"   Found {len(results)} {self.t('found_results')}")
            
            # Display top 5 key data (including rerank score)
            for i, r in enumerate(results[:5], 1):
                esg_info = r['esg_info']
                rerank_score = r.get('rerank_score', 0)
                value = esg_info.get('value', 'N/A')
                value_status = "âœ…" if value and str(value).lower() != 'nan' and str(value).strip() else "âŒ"
                print(f"   {i}. {value_status} {esg_info.get('company', 'N/A')} ({esg_info.get('year', 'N/A')}) - {esg_info.get('indicator', 'N/A')}: {value} [{self.t('quality_score')}:{rerank_score:.1f}]")
            
            if len(results) > 5:
                print(f"   ... {len(results) - 5} {self.t('more_data_available')}")
        else:
            print(f"âŒ {self.t('no_results')}")
    
    def show_help(self):
        """Display help information"""
        print(f"\nğŸ“– {self.t('usage_help')}:")
        print("=" * 50)
        print(f"ğŸ’¡ {self.t('query_examples')}:")
        print("   â€¢ A US Equity 2015 Pct Women in Workforce indicator")
        print("   â€¢ Agilent Technologies Inc 2015 women workforce percentage")
        print("   â€¢ ES047 indicator data")
        print("   â€¢ 2015 environmental emissions trend")
        print("   â€¢ How about this trend? (contextual query)")
        print(f"\nğŸ”§ {self.t('commands')}:")
        print(f"   â€¢ {self.t('help')} - Display help information")
        print(f"   â€¢ {self.t('collections')} - Display available data collections")
        print(f"   â€¢ {self.t('memory')} - Display memory report")
        print(f"   â€¢ {self.t('clear')} - Clear memory")
        print(f"   â€¢ {self.t('mode')} - Toggle response mode (LLM/Basic)")
        print(f"   â€¢ {self.t('debug')} - Enable/disable debug mode")
        print(f"   â€¢ {self.t('quit')} - Exit system")
        print(f"\nğŸ’­ {self.t('tips')}:")
        print(f"   â€¢ {self.t('supports_natural_language')}")
        print(f"   â€¢ {self.t('supports_multilingual')}")
        print(f"   â€¢ {self.t('supports_context')}")
        print(f"   â€¢ {self.t('generates_smart_responses')}")
        print(f"   â€¢ {self.t('remembers_history')}")
        print(f"   â€¢ {self.t('auto_detects_collections')}")
    
    def show_collections(self):
        """Display available data collections"""
        print(f"\nğŸ“Š {self.t('available_collections')}:")
        print("=" * 50)
        
        if not self.collections:
            print(f"âŒ {self.t('no_collections_found')}")
            return
        
        for i, collection in enumerate(self.collections, 1):
            try:
                count = collection.count()
                print(f"{i}. {self.t('collection_name')}: {collection.name}")
                print(f"   ğŸ“ˆ {self.t('record_count')}: {count}")
                
                # Get sample data
                sample = collection.get(limit=1, include=['metadatas'])
                if sample['metadatas']:
                    metadata = sample['metadatas'][0]
                    print(f"   ğŸ“‹ {self.t('sample_metadata')}: {metadata}")
                
                print()
                
            except Exception as e:
                print(f"{i}. {self.t('collection_name')}: {collection.name}")
                print(f"   âš ï¸ {self.t('failed_to_get_info')}: {str(e)}")
                print()
    
    def show_memory_report(self):
        """Display memory report"""
        if not self.memory['queries']:
            print(f"\nğŸ“ {self.t('memory_empty')}")
            return
        
        # Statistics
        total_queries = len(self.memory['queries'])
        recent_queries = [q for q in self.memory['queries'] if 
                         (datetime.now() - datetime.fromisoformat(q['timestamp'])).days < 7]
        
        # Popular companies
        all_companies = []
        for query in self.memory['queries']:
            all_companies.extend(query['companies'])
        company_counts = pd.Series(all_companies).value_counts()
        
        # Popular years
        all_years = []
        for query in self.memory['queries']:
            all_years.extend(query['years'])
        year_counts = pd.Series(all_years).value_counts()
        
        print(f"\nğŸ“Š {self.t('memory_report')}:")
        print("=" * 50)
        print(f"{self.t('total_queries')}: {total_queries}")
        print(f"{self.t('recent_queries')}: {len(recent_queries)}")
        print(f"{self.t('popular_companies')}: {', '.join(company_counts.head(5).index.tolist())}")
        print(f"{self.t('popular_years')}: {', '.join(map(str, year_counts.head(5).index.tolist()))}")
        print(f"{self.t('pattern_count')}: {len(self.memory['patterns'])}")
        print(f"{self.t('trend_count')}: {len(self.memory['trends'])}")
    
    def clear_memory(self):
        """Clear memory"""
        self.memory = {
            'queries': [],
            'results': [],
            'patterns': {},
            'insights': [],
            'trends': {},
            'last_update': None
        }
        self.save_memory()
        print(f"\nğŸ—‘ï¸ {self.t('memory_cleared')}")
    
    def interactive_mode(self):
        """Interactive mode"""
        print(f"\nğŸš€ {self.t('welcome')}")
        print("=" * 60)
        print(f"ğŸ’¡ {self.t('enter_query')}")
        print(f"ğŸ’¡ {self.t('quit')} to exit system")
        print("=" * 60)
        
        while True:
            try:
                # Get user input
                user_input = input(f"\nğŸ” {self.t('enter_query_prompt')}: ").strip()
                
                # Handle special commands
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print(f"\nğŸ‘‹ {self.t('goodbye')}")
                    break
                elif user_input.lower() in ['help', 'å¸®åŠ©']:
                    self.show_help()
                    continue
                elif user_input.lower() in ['collections', 'é›†åˆ']:
                    self.show_collections()
                    continue
                elif user_input.lower() in ['memory', 'è®°å¿†']:
                    self.show_memory_report()
                    continue
                elif user_input.lower() in ['clear', 'æ¸…ç©º']:
                    self.clear_memory()
                    continue
                elif user_input.lower() in ['mode', 'æ¨¡å¼']:
                    self.toggle_llm_mode()
                    continue
                elif user_input.lower() in ['debug', 'è°ƒè¯•']:
                    self.debug_mode = not self.debug_mode
                    status = self.t('debug_mode_on') if self.debug_mode else self.t('debug_mode_off')
                    print(f"ğŸ” {status}")
                    continue
                elif not user_input:
                    print(f"âŒ {self.t('invalid_query')}")
                    continue
                
                # Execute query
                result = self.intelligent_query(user_input)
                self.display_results(result)
                
            except KeyboardInterrupt:
                print(f"\n\nğŸ‘‹ {self.t('goodbye')}")
                break
            except Exception as e:
                print(f"\nâŒ {self.t('processing_error')}: {str(e)}")
                print(f"ğŸ’¡ {self.t('try_again')}")

def main():
    """Main function - Smart Interactive MemoRAG"""
    
    # Use relative path to ensure code can run in different environments
    db_path = "."  # Current directory
    
    print("ğŸš€ Smart MemoRAG + BGE-M3 ESG System")
    print("=" * 60)
    
    # Language selection
    print("\nğŸŒ Language Selection / è¯­è¨€é€‰æ‹©")
    print("=" * 40)
    print("1. English")
    print("2. Chinese (ä¸­æ–‡)")
    
    while True:
        lang_choice = input("\nPlease select language / è¯·é€‰æ‹©è¯­è¨€ (1/2): ").strip()
        if lang_choice == "1":
            language = "en"
            break
        elif lang_choice == "2":
            language = "zh"
            break
        else:
            print("Invalid choice. Please enter 1 or 2. / æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1æˆ–2ã€‚")
    
    print(f"\nâœ… Language set to {'English' if language == 'en' else 'Chinese (ä¸­æ–‡)'}")
    print("=" * 60)
    
    # Ask whether to use LLM
    use_llm = input("ğŸ¤– Use LLM to generate smart responses? (y/n): ").strip().lower()
    llm_api_key = None
    
    if use_llm == 'y':
        llm_api_key = input("ğŸ”‘ Enter DeepSeek API Key: ").strip()
        if not llm_api_key:
            print("âš ï¸ No API Key provided, using basic response mode")
            llm_api_key = None
    
    # Initialize system
    memorag = FixedMemoRAG(db_path, llm_api_key=llm_api_key, language=language)
    
    # Start interactive mode
    memorag.interactive_mode()

if __name__ == "__main__":
    main()
